{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kesci_text.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WRcqGs-hKCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import word2vec\n",
        "from sklearn.preprocessing import normalize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0BpVrqcn_nU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 导入数据\n",
        "filename = 'C:/Users/Youmin/Desktop/train_data.sample'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MOxGMRzTds7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step1：\n",
        "# 数据集基本特征dataframe表格化制作\n",
        "data_ = []\n",
        "with open(filename,'r') as f:\n",
        "    \n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        line = line.split(',')\n",
        "        line[0] = int(line[0]) # Query_id\n",
        "        \n",
        "        line[1] = line[1].split(' ') # Query\n",
        "        line.append(len(line[1])) # line[5] Query_length\n",
        "        \n",
        "        line[2] = int(line[2]) # Query title id\n",
        "        \n",
        "        line[3] = line[3].split(' ') # title\n",
        "        line.append(len(line[3])) # line[6] title_length\n",
        "        line[4] = int(line[4].replace('\\t\\n','')) #label以及换行缩进符清洗\n",
        "        data_.append(line)\n",
        "# 至此，数据包含特征如下6个，除标签\n",
        "data = pd.DataFrame(data_,columns=['query_id', 'query', 'query_title_id', 'title','label','query_length', 'title_length'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AA2BZz0Tnua",
        "colab_type": "code",
        "outputId": "c0151c96-d9d5-4f57-be31-cfed5ff4acb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# 统计特征扩充，groupby扩展\n",
        "# 统计1：以label和query长度为基础进行划分，统计不同点击情况下不同query长度对title_length的影响\n",
        "grouped_length = data['title_length'].groupby([data['label'],data['query_length']])\n",
        "groupby_mean_mat = grouped_length.mean().unstack()\n",
        "\n",
        "# 统计2：基于query_id的点击率以label和query_id作为基础进行划分，看看不同id被统计的概率是多少\n",
        "grouped_click_id = data['query_id'].groupby([data['label'],data['query_id']])\n",
        "groupby_click_mat = grouped_click_id.count().unstack().fillna(0)# 同时进行缺省值处理\n",
        "clicked = groupby_click_mat.loc[1] # 被点击的数量\n",
        "groupby_click_sum = groupby_click_mat.sum()\n",
        "prob_click_id = round(clicked /groupby_click_sum,3)# 保留小数点3位\n",
        "def click_prob(x):\n",
        "    return prob_click_id[x]\n",
        "def group_sum(x):\n",
        "    return groupby_click_sum[x]\n",
        "data['click_prob'] = data['query_id'].map(click_prob)\n",
        "data['group_sum'] = data['query_id'].map(group_sum)\n",
        "\n",
        "# 这里是一个粗概率，目前不考虑用标准化处理，希望模型可以通过参数自动学习\n",
        "data['0_prob_titlen'] = data['query_length'].map(lambda x: groupby_mean_mat[x][0])\n",
        "data['1_prob_titlen'] = data['query_length'].map(lambda x: groupby_mean_mat[x][1])\n",
        "\n",
        "    # 构建groupby表格，考虑对列做标准化处理，表格表示形式如下：\n",
        "#    query_length        1          2          3    ...   55    198        300\n",
        "#    label                                          ...                       \n",
        "#    0             12.868735  13.179517  13.449843  ...  17.0  11.0  14.300000\n",
        "#    1             13.296875  13.525405  13.538568  ...  12.0   4.0  13.666667\n",
        "# 至此，统计特征制作完毕"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b7c2da3b4bd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP6eMNtLdd7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#基于embedding的距离特征\n",
        "'''\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from scipy.linalg import norm\n",
        "import gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1EL5jbvWJe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1.杰卡德系数计算\n",
        "def jaccard_similarity(s1,s2):\n",
        "    def add_space(s):\n",
        "        return ' '.join(list(s))\n",
        "    s1,s2 = add_space(s1),add_space(s2)\n",
        "    cv = CountVectorizer(tokenizer=lambda s: s.split())\n",
        "    corpus = [s1,s2]\n",
        "    vectors = cv.fit_transform(corpus).toarray()\n",
        "    numerator = np.sum(np.min(vectors, axis = 0))\n",
        "    denominator = np.sum(np.max(vectors, axis = 0))\n",
        "    return 1.0* numerator/denominator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdG44E_aWK20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2.TF距离计算\n",
        "def tf_similarity(s1, s2):\n",
        "    def add_space(s):\n",
        "        return ' '.join(list(s))\n",
        "    \n",
        "    s1, s2 = add_space(s1), add_space(s2)\n",
        "    cv = CountVectorizer(tokenizer = lambda s: s.split())\n",
        "    corpus = [s1,s2]\n",
        "    vectors = cv.fit_transform(corpus).toarray()\n",
        "    return np.dot(vectors[0], vectors[1])/(norm(vectors[0])*norm(vectors[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtjGlpHSWMAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3. TF-IDF计算\n",
        "def tfidf_similarity(s1, s2):\n",
        "    def add_space(s):\n",
        "        return ' '.join(list(s))\n",
        "    s1,s2 = add_space(s1),add_space(s2)\n",
        "    cv = TfidfVectorizer(tokenizer=lambda s: s.split())\n",
        "    corpus = [s1,s2]\n",
        "    vectors = cv.fit_transform(corpus).toarray()\n",
        "    return np.dot(vectors[0],vectors[1])/(norm(vectors[0])*norm(vectors[1]))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSGs2RrvWOvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4. word2vec计算\n",
        "#导入模型\n",
        "#model_file = 'C:/Users/Youmin/Desktop/w2vmodel_title'\n",
        "#model = gensim.models.Word2Vec.load(model_file)\n",
        "#\n",
        "#def vector_similariy(s1,s2):\n",
        "#    def sentence_vector(s):\n",
        "#        words = 原来用的结巴.lcut(s)\n",
        "#        v = np.zeros(64)\n",
        "#        for word in words:\n",
        "#            v += model[word]\n",
        "#        v /= len(words)\n",
        "#        return v\n",
        "#    v1, v2 = sentence_vector(s1), sentence_vector(s2)\n",
        "#    return np.dot(v1, v2) / (norm(v1)*norm(v2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz2fOdytWR9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(x):\n",
        "    # 将标签为1的各个query_id找出来匹配上，输入的m是query_id的下标\n",
        "    l = data[(data['query_id']==m)&(data['label']==1)].index.tolist() #这里l返回的是data中index下标\n",
        "    jaccard_dist = []\n",
        "    tf_dist = []\n",
        "    tfidf_dist = []\n",
        "    for i in range(len(l)):\n",
        "        #jaccard\n",
        "        j_dist = jaccard_similarity(x,data['title'][l[i]])\n",
        "        jaccard_dist.append(j_dist)\n",
        "        \n",
        "        #TF\n",
        "        TF_dist = tf_similarity(x,data['title'][l[i]])\n",
        "        tf_dist.append(TF_dist)\n",
        "        \n",
        "        #TF-IDF\n",
        "        TfIdf_dist = tfidf_similarity(x,data['title'][l[i]])\n",
        "        tfidf_dist.append(TfIdf_dist)\n",
        "        \n",
        "    jaccard = np.mean(jaccard_dist)\n",
        "    tf_ = np.mean(tf_dist)\n",
        "    tfidf = np.mean(tfidf_dist)\n",
        "    return jaccard,tf_,tfidf\n",
        "    #返回三个距离均值\n",
        "    \n",
        "#因此这里的关键是找出m\n",
        "temp = pd.Series()\n",
        "for m in range(len(data['query_id'].groupby(data['query_id']).count())):\n",
        "    m = m+1\n",
        "    dist_seq = data['title'][(data['query_id']==m)].map(f)# 这是x\n",
        "    temp = pd.concat([temp,dist_seq])\n",
        "#    jac = dist_seq['title'].map(lambda x:x[0])\n",
        "#    tf = dist_seq['title'].map(lambda x:x[1])\n",
        "#    idf = dist_seq['title'].map(lambda x:x[2])\n",
        "#    \n",
        "#    pd.concat([jac,tf,idf],axis=1)\n",
        "i = 0\n",
        "data['jaccard_dist'] = temp.map(lambda x: x[i])\n",
        "i = 1\n",
        "data['tf_dist'] = temp.map(lambda x: x[i])\n",
        "i = 2\n",
        "data['tfidf_dist'] = temp.map(lambda x: x[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LokMVfAWTsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 针对数值型数据占用内存 减少内存调用方法\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)    \n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDIANGdc704I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 语料特征构建\n",
        "\n",
        "from gensim import corpora\n",
        "from gensim import models\n",
        "\n",
        "# 设置构建语料库的数据集大小\n",
        "'''\n",
        "  在这里设置语料库的提取材料大小\n",
        "'''\n",
        "texts = data['title']# [给他一个范围]\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "tfidf_corpus = models.TfidfModel(corpus)\n",
        "# 至此语料库就是tfidf_corpus\n",
        "# tfidf_corpus[xxxxx]\n",
        "#def corpus_map(x):\n",
        "#    l = dictionary.doc2bow(x)\n",
        "#    return tfidf_corpus[l]\n",
        "#data['title_corpus'] = data['title'].map(corpus_map)\n",
        "\n",
        "\n",
        "# LDA corpus\n",
        "# set the topic words components\n",
        "num_topic = 100\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "lda = LdaModel(corpus,num_topics=num_topic)\n",
        "def lda_map(x):\n",
        "    l = dictionary.doc2bow(x)\n",
        "    return lda[l]\n",
        "temp_ = data['title'].map(lda_map) # 这里是对整个数据集做映射\n",
        "temp_ = pd.DataFrame(dict([ (k,pd.Series(v).map(list)) for k,v in temp_.iteritems() ])).fillna(0).T#\n",
        "l_ = list(range(num_topic))\n",
        "l = [str(x) for x in l_]\n",
        "temp_.columns = l\n",
        "\n",
        "for i in range(len(temp_)):\n",
        "    temp_.iloc[i] = temp_.iloc[i].map(lambda x: x[1] if x!=0 else 0)\n",
        "\n",
        "data = pd.concat([data,temp_],axis=1)\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "  至此所有数据特征构建完毕\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMjMbMU0700w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "\n",
        "记得划分数据集用来提炼语料库，大致为1000万文本提100个关键词\n",
        "\n",
        "其次是训练过程中，将1亿样本分批抽样训练10组，每组1000+100\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wevBk_vV70xQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 模型输出部分\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_input = data.drop(['label','query','title'],axis=1).values\n",
        "target = data['label'].values\n",
        "\n",
        "X_train,X_dev,y_train,y_dev = train_test_split(data_input,target,test_size=0.2)\n",
        "\n",
        "\n",
        "# lightgbm\n",
        "param = {\n",
        "    'num_leaves':150, \n",
        "    'objective':'binary',\n",
        "    'max_depth':7,\n",
        "    'learning_rate':.05,\n",
        "    'max_bin':200}\n",
        "param['metric'] = ['auc', 'binary_logloss']\n",
        "\n",
        "train_data = lgb.Dataset(X_train,label=y_train)\n",
        "\n",
        "num_round = 50\n",
        "lgbm = lgb.train(param,train_data,num_round)\n",
        "\n",
        "\n",
        "ypred2 = lgbm.predict(X_dev)\n",
        "\n",
        "output = pd.DataFrame()\n",
        "output['query_id'] = X_dev[:,0]\n",
        "output['query_title_id'] = X_dev[:,1]\n",
        "output['prediction'] = ypred2\n",
        "output = output.sort_values(by=['query_id','query_title_id'])\n",
        "output.index = range(len(output))\n",
        "\n",
        "output.to_csv('.test.csv',index=False,header=False,sep=\",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kde6gCNn70ty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvDZbdZP70qg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}